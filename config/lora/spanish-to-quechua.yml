method: lora
wandb_project: XGLM-564M-LoRA-Finetuning
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
epochs: 5
learning_rate: 0.00005

dataset:
  train:
    path: hackathon-pln-es/spanish-to-quechua
    name:
    split: train

tokenizer:
  max_length: 512