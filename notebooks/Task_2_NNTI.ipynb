{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Saving the hidden representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# %pip install datasets transformers\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"USING DEVICE: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_NAME = \"facebook/xglm-564M\"\n",
    "\n",
    "# For task 2\n",
    "TASK = \"task_2\"\n",
    "MODEL_NAME = \"facebook/xglm-564M\"\n",
    "\n",
    "# For task 3\n",
    "# TASK = \"task_3\"\n",
    "# MODEL_NAME = \"../models/full-nllb.pt\"\n",
    "\n",
    "DATASET_NAME = \"facebook/flores\"\n",
    "\n",
    "NUM_LAYERS = 25\n",
    "SAMPLES = 200\n",
    "\n",
    "AVAILABLE_SPLITS = [\"dev\", \"devtest\"]\n",
    "\n",
    "# this is the minimal set of languages that you should analyze\n",
    "# feel free to experiment with additional lanuages available in the flores dataset\n",
    "LANGUAGES = [\n",
    "    \"eng_Latn\",\n",
    "    \"spa_Latn\",\n",
    "    \"deu_Latn\",\n",
    "    \"arb_Arab\",\n",
    "    \"tam_Taml\",\n",
    "    \"quy_Latn\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's handle the random sampling of the dataset\n",
    "def random_sample(dataset, n):\n",
    "    \"\"\" Randomly sample n sentences from the dataset for a language\n",
    "\n",
    "    Args:\n",
    "        dataset: torch.utils.data.Dataset - the dataset to sample from\n",
    "        n: int - the number of samples to draw\n",
    "    \n",
    "    Returns:\n",
    "        torch.utils.data.Subset - a subset of the original dataset with n samples\n",
    "    \"\"\"\n",
    "    dataset_len = len(dataset)\n",
    "    if n > dataset_len:\n",
    "        raise ValueError(\"Number of samples cannot exceed the dataset length.\")\n",
    "\n",
    "    indices = np.sort(np.random.choice(dataset_len, n, replace=False))\n",
    "    return torch.utils.data.Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flores_datasets(languages, splits):\n",
    "    \"\"\" Loads the FLORES datasets for the specified languages and splits\n",
    "\n",
    "    Args:\n",
    "        languages (list): a list of languages\n",
    "        splits (list): a list of splits\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary of datasets for each language and split\n",
    "    \"\"\"\n",
    "    flores_data = {}\n",
    "    for language in languages:\n",
    "        print(f\"Loading dataset for language: {language}\")\n",
    "        flores_data[language] = {}\n",
    "        for split in splits:\n",
    "            flores_data[language][split] = {}\n",
    "            flores_data[language][split] = load_dataset(\n",
    "                \"facebook/flores\",\n",
    "                language,\n",
    "                split=split,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=\"../cache/languages\"\n",
    "            )\n",
    "\n",
    "    for language in LANGUAGES:\n",
    "        for split in AVAILABLE_SPLITS:\n",
    "            print(f\"Sampling {SAMPLES} samples for {language} - {split}\")\n",
    "            flores_data[language][split] = random_sample(flores_data[language][split], SAMPLES)\n",
    "    return flores_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\" Tokenizer class to tokenize a given example for a given model\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, padding=\"longest\", truncation=\"longest_first\", return_tensors=\"pt\"):\n",
    "        self.model_name = model_name\n",
    "        self.padding = padding\n",
    "        self.truncation = truncation\n",
    "        self.return_tensors = return_tensors\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize(self, sentences):\n",
    "        \"\"\"Tokenizes the given input text\n",
    "\n",
    "        Args:\n",
    "            sentences (list): The input text to be tokenized\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the tokenized input text, attention mask, and labels\n",
    "        \"\"\"\n",
    "        tokenized = self.tokenizer(\n",
    "            sentences,\n",
    "            padding=self.padding,\n",
    "            return_tensors=self.return_tensors,\n",
    "            truncation=self.truncation\n",
    "        )\n",
    "\n",
    "        # It's more efficient to just return the enitre sentence and tokens from here\n",
    "        tokenized[\"sentence\"] = [sentence for sentence in sentences]\n",
    "        tokenized[\"tokens\"] = [self.tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "        return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer):\n",
    "    \"\"\" Collate function to convert a batch of samples into a batch of padded tokenized sequences\n",
    "\n",
    "    Args:\n",
    "        batch (list): a list of samples\n",
    "        tokenizer (Tokenizer): the tokenizer\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary of tokenized sequences\n",
    "    \"\"\"\n",
    "    return tokenizer.tokenize([sample[\"sentence\"] for sample in batch])\n",
    "\n",
    "def build_dataloaders(languages, batch_size, collate_fn, tokenizer, shuffle=False):\n",
    "    \"\"\" Builds dataloaders for a given set of languages and tokenizer using the specified batch size and collate function\n",
    "\n",
    "    Args:\n",
    "        languages (list): a list of languages\n",
    "        batch_size (int): the batch size\n",
    "        collate_fn (function): the collate function\n",
    "        tokenizer (Tokenizer): the tokenizer\n",
    "        shuffle (bool, optional): whether to shuffle the dataset. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary of dataloaders for each language\n",
    "    \"\"\"\n",
    "    flores_data = load_flores_datasets(languages, [\"dev\", \"devtest\"])\n",
    "\n",
    "    flores_dataloaders = {}\n",
    "    for language in languages:\n",
    "        flores_dataloaders[language] = DataLoader(\n",
    "            flores_data[language][\"devtest\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=lambda batch: collate_fn(batch, tokenizer)\n",
    "        )\n",
    "    return flores_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_data_to_file(data, lang):\n",
    "    \"\"\" Writes the data to a hdf5 file\n",
    "\n",
    "    Args:\n",
    "        data (dict): The data to write to the file\n",
    "        lang (str): The name of the file\n",
    "    \"\"\"\n",
    "    with h5py.File(f'../data/{TASK}/{lang}.hdf5', 'w') as f:\n",
    "      for key, value in data.items():\n",
    "        main_group = f.create_group(key)\n",
    "        for field, value in value.items():\n",
    "          if field == \"sentences\": \n",
    "            main_group.create_dataset(field, data=value)\n",
    "          else:\n",
    "            layers_group = main_group.create_group(field) # Create the layers group\n",
    "            for layer_key, layer_val in value.items(): \n",
    "              layer_group = layers_group.create_group(layer_key) # Create a group for each layer\n",
    "              for sentence_key, sentence_value in layer_val.items():\n",
    "                sentence_group = layer_group.create_group(sentence_key) # Create a group for each sentence\n",
    "                for token_key, token_value in sentence_value.items():\n",
    "                  if token_key == \"s\": # The hidden state of the entire sentence\n",
    "                    sentence_group.create_dataset(token_key, data=token_value)                    \n",
    "                  else:\n",
    "                    token_group = sentence_group.create_group(token_key) # Create a group for each token\n",
    "                    for item_key, item_value in token_value.items():\n",
    "                      token_group.create_dataset(item_key, data=item_value)\n",
    "      f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_name, device):\n",
    "    \"\"\"Builds a model from a given model name and device\n",
    "\n",
    "    Args:\n",
    "        model_name (str): the name or path of the model\n",
    "        device (torch.device): the device to run the model on\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: the model\n",
    "    \"\"\"\n",
    "    if os.path.exists(model_name):\n",
    "        print(f\"Loading model from path: {model_name}\")\n",
    "        model = torch.load(model_name)\n",
    "    else:\n",
    "        print(f\"Loading model from name: {model_name}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "tokenizer = Tokenizer(TOKENIZER_NAME)\n",
    "\n",
    "flores_dataloaders = build_dataloaders(LANGUAGES, BATCH_SIZE, collate_fn, tokenizer)\n",
    "\n",
    "model = build_model(MODEL_NAME, device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Data format stored in file\n",
    "    data = {\n",
    "      \"sentences\": [\"My cat has an orange cat energy\", \"My dog is cute\"], # The list of sampled sentences\n",
    "      \"layers\": {                                                         # A dictionary of layers\n",
    "        \"l_1\": {                                                          # The first layer\n",
    "          \"s_0\": {                                                        # Sentence first sentence\n",
    "            \"t_0\": {                                                      # The first token of the first sentence\n",
    "              \"t\": str,                                                   # The string representation of the token\n",
    "              \"s\": np.array                                               # Hidden representation of the token\n",
    "              \"id\": int                                                   # The encoded id of the token\n",
    "            },\n",
    "            \"t_1\": {\n",
    "              \"t\": str,\n",
    "              \"s\": np.array,\n",
    "              \"id\": int\n",
    "            },\n",
    "            \"s\": np.array,                                                # Hidden representation of the entire sentence\n",
    "          },\n",
    "        \"l_2\": {\n",
    "          \"s_0\": {\n",
    "            \"t_0\": {\n",
    "              \"t\": str,\n",
    "              \"s\": np.array,\n",
    "              \"id\": int\n",
    "            },\n",
    "            \"t_1\": {\n",
    "              \"t\": str,\n",
    "              \"s\": np.array,\n",
    "              \"id\": int\n",
    "            },\n",
    "            \"s\": np.array,\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "'''\n",
    "with torch.no_grad():\n",
    "  for idx_lang, lang in enumerate(LANGUAGES):\n",
    "    data = {}\n",
    "    data[\"data\"] = {}\n",
    "    data[\"data\"][\"layers\"] = {}\n",
    "    data[\"data\"][\"sentences\"] = []\n",
    "\n",
    "    # Initialize the layers and sentences\n",
    "    for idx_layer in range(NUM_LAYERS):\n",
    "      data[\"data\"][\"layers\"][f\"l_{idx_layer}\"] = {}\n",
    "      for idx_sentence in range(SAMPLES):\n",
    "        data[\"data\"][\"layers\"][f\"l_{idx_layer}\"][f\"s_{idx_sentence}\"] = {}\n",
    "\n",
    "    for idx_batch, batch in enumerate(tqdm(flores_dataloaders[lang])):\n",
    "      data[\"data\"][\"sentences\"].extend(batch[\"sentence\"])\n",
    "\n",
    "      input_ids = batch[\"input_ids\"].to(device)\n",
    "      attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "      outputs = model.forward(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "          output_hidden_states=True\n",
    "      )\n",
    "\n",
    "      # h_l -> hidden layer\n",
    "      for idx_h_l, h_l in enumerate(outputs.hidden_states):\n",
    "        idx_sentence = idx_batch * BATCH_SIZE\n",
    "        for id_sequence, token_sequence in enumerate(batch[\"input_ids\"]):\n",
    "          tokens_per_seq = []\n",
    "          for idx_token, token in enumerate(token_sequence):\n",
    "            if token == tokenizer.tokenizer.pad_token_id:\n",
    "              continue\n",
    "\n",
    "            state = h_l[id_sequence][idx_token].cpu()\n",
    "            tokens_per_seq.append(state)\n",
    "\n",
    "            data[\"data\"][\"layers\"][f\"l_{idx_h_l}\"][f\"s_{idx_sentence + id_sequence}\"][f\"t_{idx_token}\"] = {\n",
    "                \"t\": tokenizer.tokenizer.convert_ids_to_tokens([token])[0],\n",
    "                \"s\": np.array(state),\n",
    "                \"id\": token.item()\n",
    "            }\n",
    "\n",
    "          stacked_tensors = torch.stack(tokens_per_seq)\n",
    "          mean = np.array(torch.mean(stacked_tensors, dim=0, keepdim=True))\n",
    "          data[\"data\"][\"layers\"][f\"l_{idx_h_l}\"][f\"s_{idx_sentence + id_sequence}\"][\"s\"] = mean\n",
    "\n",
    "    # Write the data to files \n",
    "    write_data_to_file(data, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Visualizing the data using PCA and t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Util methods for performing PCA and t-SNE on some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def apply_pca(data, n_components=2):\n",
    "    \"\"\"Applies PCA to the given data\n",
    "\n",
    "    Args:\n",
    "        data (MatrixLike): The data to be transformed\n",
    "        n_components (int, optional): Number of components to keep. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Data transformed by PCA\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(data)\n",
    "\n",
    "def apply_tsne(data, n_components=2, perplexity=30, n_jobs=-1):\n",
    "    \"\"\"Applies t-SNE to the given data\n",
    "\n",
    "    Args:\n",
    "        data (MatrixLike): The data to be transformed\n",
    "        n_components (int, optional): Number of components to keep. Defaults to 2.\n",
    "        learning_rate (str, optional): The learning rate for t-SNE. Defaults to 'auto'.\n",
    "        init (str, optional): Initialization of embedding. Defaults to 'random'.\n",
    "        n_iter (int, optional): Maximum number of iterations for the optimization. Defaults to 1000.\n",
    "        perplexity (int, optional): The perplexity is related to the number of nearest neighbors. Defaults to 40.\n",
    "        n_jobs (int, optional): The number of parallel jobs to run. Defaults to -1.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Data transformed by t-SNE\n",
    "    \"\"\"\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, n_jobs=n_jobs)\n",
    "    return tsne.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrive the hidden representation from the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER = 1 # Not 0 because we want to visualize the first hidden layer and not the embedding layer\n",
    "\n",
    "data = []\n",
    "for lang in LANGUAGES:\n",
    "    f = h5py.File(f'../data/{TASK}/{lang}.hdf5', 'r')\n",
    "    for idx_sample in range(SAMPLES):\n",
    "        for idx_token in range(len(f[f\"data/layers/l_{LAYER}/s_{idx_sample}\"]) - 1): # Do not include the hidden state of the entire sentence\n",
    "          data.append({\n",
    "              \"token\": f[f\"data/layers/l_{LAYER}/s_{idx_sample}/t_{idx_token}/t\"][()].decode(\"utf-8\"),\n",
    "              \"hidden_state\": f[f\"data/layers/l_{LAYER}/s_{idx_sample}/t_{idx_token}/s\"][()],\n",
    "              \"language\": lang\n",
    "          })\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply PCA to the hidden representations of each language of the first layer of the model and visualizing the result in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataframe\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "main_df = pd.DataFrame(columns=[\"First component\", \"Second component\", \"Token\", \"Language\"])\n",
    "pca_data = apply_pca([d[\"hidden_state\"] for d in data])\n",
    "df = pd.DataFrame(pca_data, columns=[\"First component\", \"Second component\"])\n",
    "df[\"Token\"] = [str(d[\"token\"]) for d in data]\n",
    "df[\"Language\"] = [d[\"language\"] for d in data]\n",
    "main_df = pd.concat([df, df])\n",
    "\n",
    "alt.Chart(main_df).mark_circle(size=20).encode(\n",
    "    x='First component',\n",
    "    y='Second component',\n",
    "    color='Language',\n",
    "    tooltip=['Token', 'Language'],\n",
    ").interactive().properties(title=\"PCA Visualization of first hidden layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply t-SNE to the hidden representations of each language of the first layer of the model and visualizing the result in 2D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataframe\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "main_df = pd.DataFrame(columns=[\"First component\", \"Second component\", \"Token\", \"Language\"])\n",
    "pca_data = apply_tsne(np.array([d[\"hidden_state\"] for d in data]))\n",
    "df = pd.DataFrame(pca_data, columns=[\"First component\", \"Second component\"])\n",
    "df[\"Token\"] = [str(d[\"token\"]) for d in data]\n",
    "df[\"Language\"] = [d[\"language\"] for d in data]\n",
    "main_df = pd.concat([df, df])\n",
    "\n",
    "alt.Chart(main_df).mark_circle(size=20).encode(\n",
    "    x='First component',\n",
    "    y='Second component',\n",
    "    color='Language',\n",
    "    tooltip=['Token', 'Language']\n",
    ").interactive().properties(title=\"t-SNE Visualization of first hidden layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the directories to store the charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dirs for the tokens charts\n",
    "tokens_path = f\"../data/{TASK}/charts/tokens\"\n",
    "sentence_path = f\"../data/{TASK}/charts/sentence\"\n",
    "\n",
    "paths_to_create = [\n",
    "    f\"{tokens_path}/png/pca\",\n",
    "    f\"{tokens_path}/svg/pca\",\n",
    "    f\"{tokens_path}/html/pca\",\n",
    "    f\"{tokens_path}/png/tsne\",\n",
    "    f\"{tokens_path}/svg/tsne\",\n",
    "    f\"{tokens_path}/html/tsne\",\n",
    "    f\"{sentence_path}/png/pca\",\n",
    "    f\"{sentence_path}/svg/pca\",\n",
    "    f\"{sentence_path}/html/pca\",\n",
    "    f\"{sentence_path}/png/tsne\",\n",
    "    f\"{sentence_path}/svg/tsne\",\n",
    "    f\"{sentence_path}/html/tsne\"\n",
    "]\n",
    "\n",
    "for path in paths_to_create:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing (PCA and tSNE) for each layer of the model (including the embedding layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_layer in range(NUM_LAYERS):\n",
    "    data = []\n",
    "\n",
    "    # Initialize the dataframe\n",
    "    main_df_pca = pd.DataFrame(columns=[\"First component\", \"Second component\", \"Token\", \"Language\"])\n",
    "    main_df_tsne = pd.DataFrame(columns=[\"First component\", \"Second component\", \"Token\", \"Language\"])\n",
    "\n",
    "    # Load the data for each language for the current `idx_layer` hidden layer\n",
    "    for lang in LANGUAGES:\n",
    "        f = h5py.File(f'../data/{TASK}/{lang}.hdf5', 'r')\n",
    "        for idx_sample in range(SAMPLES):\n",
    "            # We need to subtract 1 because the last element is the hidden representation of the entire sentence \n",
    "            for idx_token in range(len(f[f\"data/layers/l_{idx_layer}/s_{idx_sample}\"]) - 1):\n",
    "                data.append({\n",
    "                    \"token\": f[f\"data/layers/l_{idx_layer}/s_{idx_sample}/t_{idx_token}/t\"][()].decode(\"utf-8\"),\n",
    "                    \"hidden_state\": f[f\"data/layers/l_{idx_layer}/s_{idx_sample}/t_{idx_token}/s\"][()],\n",
    "                    \"language\": lang\n",
    "                })\n",
    "        f.close()\n",
    "        \n",
    "    pca_data = apply_pca([d[\"hidden_state\"] for d in data])\n",
    "    df = pd.DataFrame(pca_data, columns=[\"First component\", \"Second component\"])\n",
    "    df[\"Token\"] = [str(d[\"token\"]) for d in data]\n",
    "    df[\"Language\"] = [d[\"language\"] for d in data]\n",
    "    main_df_pca = pd.concat([main_df_pca, df])\n",
    "\n",
    "    tsne_data = apply_tsne(np.array([d[\"hidden_state\"] for d in data]))\n",
    "    df = pd.DataFrame(tsne_data, columns=[\"First component\", \"Second component\"])\n",
    "    df[\"Token\"] = [str(d[\"token\"]) for d in data]\n",
    "    df[\"Language\"] = [d[\"language\"] for d in data]\n",
    "    main_df_tsne = pd.concat([main_df_tsne, df])\n",
    "\n",
    "    if idx_layer == 0:\n",
    "        title = \"Visualization of the embedding layer\"\n",
    "    else:\n",
    "        title = f\"Visualization of the {idx_layer} hidden layer\"\n",
    "    \n",
    "    chart_pca = alt.Chart(main_df_pca).mark_circle(size=20).encode(\n",
    "        x='First component',\n",
    "        y='Second component',\n",
    "        color='Language',\n",
    "        tooltip=['Token', 'Language'],\n",
    "    ).properties(title=f\"PCA {title}\")\n",
    "\n",
    "    chart_tsne = alt.Chart(main_df_tsne).mark_circle(size=20).encode(\n",
    "        x='First component',\n",
    "        y='Second component',\n",
    "        color='Language',\n",
    "        tooltip=['Token', 'Language'],\n",
    "    ).properties(title=f\"t-SNE {title}\")\n",
    "    \n",
    "    chart_pca.save(f'../data/{TASK}/charts/tokens/png/pca/pca_{idx_layer}.png')\n",
    "    chart_pca.save(f'../data/{TASK}/charts/tokens/svg/pca/pca_{idx_layer}.svg')\n",
    "    chart_pca.save(f'../data/{TASK}/charts/tokens/html/pca/pca_{idx_layer}.html')\n",
    "\n",
    "    chart_tsne.save(f'../data/{TASK}/charts/tokens/png/tsne/tsne_{idx_layer}.png')\n",
    "    chart_tsne.save(f'../data/{TASK}/charts/tokens/svg/tsne/tsne_{idx_layer}.svg')  \n",
    "    chart_tsne.save(f'../data/{TASK}/charts/tokens/html/tsne/tsne__{idx_layer}.html')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence representation for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_layer in tqdm(range(NUM_LAYERS)):\n",
    "    data = []\n",
    "\n",
    "    # Initialize the dataframe\n",
    "    main_df_pca = pd.DataFrame(columns=[\"First component\", \"Second component\", \"Sentence\", \"Language\"])\n",
    "    main_df_tsne = pd.DataFrame(columns=[\"First component\", \"Second component\", \"Sentence\", \"Language\"])\n",
    "\n",
    "    for lang in LANGUAGES:\n",
    "        f = h5py.File(f'../data/{TASK}/{lang}.hdf5', 'r')\n",
    "        for idx_sample in range(SAMPLES):\n",
    "            data.append({\n",
    "                \"sentence\": f[f\"data/sentences\"][idx_sample].decode(\"utf-8\"),\n",
    "                \"hidden_state\": f[f\"data/layers/l_{idx_layer}/s_{idx_sample}/s\"][()][0],\n",
    "                \"language\": lang\n",
    "            })\n",
    "        f.close()\n",
    "\n",
    "    pca_data = apply_pca([d[\"hidden_state\"] for d in data])\n",
    "    df = pd.DataFrame(pca_data, columns=[\"First component\", \"Second component\"])\n",
    "    df[\"Sentence\"] = [str(d[\"sentence\"]) for d in data]\n",
    "    df[\"Language\"] = [d[\"language\"] for d in data]\n",
    "    main_df_pca = pd.concat([main_df_pca, df])\n",
    "\n",
    "    tsne_data = apply_tsne(np.array([d[\"hidden_state\"] for d in data]))\n",
    "    df = pd.DataFrame(tsne_data, columns=[\"First component\", \"Second component\"])\n",
    "    df[\"Sentence\"] = [str(d[\"sentence\"]) for d in data]\n",
    "    df[\"Language\"] = [d[\"language\"] for d in data]\n",
    "    main_df_tsne = pd.concat([main_df_tsne, df])\n",
    "\n",
    "    if idx_layer == 0:\n",
    "        title = \"Visualization of the embedding layer\"\n",
    "    else:\n",
    "        title = f\"Visualization of the {idx_layer} hidden layer\"\n",
    "        \n",
    "    chart_pca = alt.Chart(main_df_pca).mark_circle(size=20).encode(\n",
    "        x='First component',\n",
    "        y='Second component',\n",
    "        color='Language',\n",
    "        tooltip=['Sentence', 'Language'],\n",
    "    ).properties(title=f\"PCA {title}\")\n",
    "\n",
    "    chart_tsne = alt.Chart(main_df_tsne).mark_circle(size=20).encode(\n",
    "        x='First component',\n",
    "        y='Second component',\n",
    "        color='Language',\n",
    "        tooltip=['Sentence', 'Language'],\n",
    "    ).properties(title=f\"t-SNE {title}\")\n",
    "    \n",
    "    chart_pca.save(f'../data/{TASK}/charts/sentence/png/pca/pca_{idx_layer}.png')\n",
    "    chart_pca.save(f'../data/{TASK}/charts/sentence/svg/pca/pca_{idx_layer}.svg')\n",
    "    chart_pca.save(f'../data/{TASK}/charts/sentence/html/pca/pca_{idx_layer}.html')\n",
    "\n",
    "    chart_tsne.save(f'../data/{TASK}/charts/sentence/png/tsne/tsne_{idx_layer}.png')\n",
    "    chart_tsne.save(f'../data/{TASK}/charts/sentence/svg/tsne/tsne_{idx_layer}.svg')\n",
    "    chart_tsne.save(f'../data/{TASK}/charts/sentence/html/tsne/tsne_{idx_layer}.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnti-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
