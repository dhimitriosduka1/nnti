{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh7qENAUQNFL",
        "outputId": "d8f59ed4-0817-4496-c2e4-0658dab3ee81"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# If running on Google Colab, install the required packages\n",
        "# %pip install datasets transformers\n",
        "\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the model on GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"USING DEVICE: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explore dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn3KiVM8ez1C",
        "outputId": "a51941ba-df3f-4c31-e30d-2a656f08548d"
      },
      "outputs": [],
      "source": [
        "# Specify dataset name\n",
        "DATA_SET_NAME = \"facebook/flores\"\n",
        "\n",
        "# Print the dataset description\n",
        "ds_builder = load_dataset_builder(\"facebook/flores\", \"deu_Latn\", trust_remote_code=True)\n",
        "print(f\"DESCRIPTION OF THE DATASET:\\n {ds_builder.info.description}\\n\")\n",
        "\n",
        "# Print the features (columns) of the dataset\n",
        "print(f\"FEATURRE COLUMNS OF THE DATASET:\\n {ds_builder.info.features}\\n\")\n",
        "\n",
        "# Get the available splits\n",
        "AVAILABLE_SPLITS = get_dataset_split_names(DATA_SET_NAME, \"deu_Latn\", trust_remote_code=True)\n",
        "print(f\"AVAILABLE SPLITS:\\n {AVAILABLE_SPLITS}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZkW80Jle3Hr",
        "outputId": "660628ac-ee4d-4890-a310-f29321e3ebe7"
      },
      "outputs": [],
      "source": [
        "# Specify languages\n",
        "LANGUAGES = [\n",
        "    \"eng_Latn\",\n",
        "    \"spa_Latn\",\n",
        "    \"ita_Latn\",\n",
        "    \"deu_Latn\",\n",
        "    \"arb_Arab\",\n",
        "    \"tel_Telu\",\n",
        "    \"tam_Taml\",\n",
        "    \"quy_Latn\"\n",
        "]\n",
        "\n",
        "\n",
        "def load_flores_datasets(languages, splits):\n",
        "    \"\"\" Loads the FLORES datasets for the specified languages and splits\n",
        "\n",
        "    Args:\n",
        "        languages (list): a list of languages\n",
        "        splits (list): a list of splits\n",
        "\n",
        "    Returns:\n",
        "        dict: a dictionary of datasets for each language and split\n",
        "    \"\"\"\n",
        "    flores_data = {}\n",
        "    for language in languages:\n",
        "        print(f\"Loading dataset for language: {language}\")\n",
        "        flores_data[language] = {}\n",
        "        for split in splits:\n",
        "            flores_data[language][split] = {}\n",
        "            flores_data[language][split] = load_dataset(\n",
        "                \"facebook/flores\",\n",
        "                language,\n",
        "                split=split,\n",
        "                trust_remote_code=True,\n",
        "                cache_dir=\"../cache/languages\"\n",
        "            )\n",
        "    return flores_data\n",
        "\n",
        "\n",
        "flores_data = load_flores_datasets(LANGUAGES, AVAILABLE_SPLITS)\n",
        "\n",
        "# Let's look at the English subset\n",
        "data = flores_data[\"eng_Latn\"][\"devtest\"].data\n",
        "print(f\"\\nENGLISH SUBSET(DEVTEST):\\n {data}\\n\")\n",
        "\n",
        "# Let's look at an individual sample from the dataset\n",
        "sample = flores_data[\"eng_Latn\"][\"devtest\"][0]\n",
        "print(f\"SAMPLE FROM ENGLISH SUBSET(DEVTEST):\\n {sample}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzmxxqwze8n_"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    \"\"\"Tokenizer class to tokenize a given example for a given model\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name, padding=\"longest\", truncation=\"longest_first\", return_tensors=\"pt\"):\n",
        "        self.model_name = model_name\n",
        "        self.padding = padding\n",
        "        self.truncation = truncation\n",
        "        self.return_tensors = return_tensors\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if self.model_name == \"gpt2\":\n",
        "            self.tokenizer.add_special_tokens({'pad_token': self.tokenizer.unk_token})\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes the given input text\n",
        "\n",
        "        Args:\n",
        "            text (list): The sentences to be tokenized\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the tokenized input text, attention mask, and labels\n",
        "        \"\"\"\n",
        "        tokenized = self.tokenizer(\n",
        "            text,\n",
        "            padding=self.padding,\n",
        "            return_tensors=self.return_tensors,\n",
        "            truncation=self.truncation\n",
        "        )\n",
        "        \n",
        "        # Replace the pad token with -100 so that it is not considered in the loss\n",
        "        tokenized[\"labels\"] = torch.where(\n",
        "            tokenized[\"input_ids\"] == self.tokenizer.pad_token_id,\n",
        "            -100,\n",
        "            tokenized[\"input_ids\"]\n",
        "        )\n",
        "\n",
        "        return tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataloader util functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTvnSJqzfAZH"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, tokenizer):\n",
        "    \"\"\"Collate function to convert a batch of samples into a batch of padded tokenized sequences\n",
        "\n",
        "    Args:\n",
        "        batch (list): a list of samples\n",
        "        tokenizer (Tokenizer): the tokenizer\n",
        "\n",
        "    Returns:\n",
        "        dict: a dictionary of tokenized sequences\n",
        "    \"\"\"\n",
        "    return tokenizer.tokenize([sample[\"sentence\"] for sample in batch])\n",
        "\n",
        "def build_dataloaders(languages, batch_size, collate_fn, tokenizer, shuffle=False):\n",
        "    \"\"\"Builds dataloaders for a given set of languages and tokenizer using the specified batch size and collate function\n",
        "\n",
        "    Args:\n",
        "        languages (list): a list of languages\n",
        "        batch_size (int): the batch size\n",
        "        collate_fn (function): the collate function\n",
        "        tokenizer (Tokenizer): the tokenizer\n",
        "        shuffle (bool, optional): whether to shuffle the dataset. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        dict: a dictionary of dataloaders for each language\n",
        "    \"\"\"\n",
        "    flores_dataloaders = {}\n",
        "    for language in languages:\n",
        "        flores_dataloaders[language] = DataLoader(\n",
        "            flores_data[language][\"devtest\"],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            collate_fn=lambda batch: collate_fn(batch, tokenizer)\n",
        "        )\n",
        "    return flores_dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Util method for loading a model with a given name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90Xx-VhlfDxx"
      },
      "outputs": [],
      "source": [
        "def build_model(model_name, device):\n",
        "    \"\"\"Builds a model from a given model name and device\n",
        "\n",
        "    Args:\n",
        "        model_name (str): the name or path of the model\n",
        "        device (torch.device): the device to run the model on\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: the model\n",
        "    \"\"\"\n",
        "    if os.path.exists(model_name):\n",
        "        print(f\"Loading model from path: {model_name}\")\n",
        "        model = torch.load(model_name)\n",
        "    else:\n",
        "        print(f\"Loading model from name: {model_name}\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Util method for running the model on INFERENCE mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAt7YF60fIUD"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def inference(model_name, tokenizer_name, batch_size, device):\n",
        "    \"\"\"Runs inference for a given model and returns the losses for each language\n",
        "\n",
        "    Args:\n",
        "        model_name (str): the name of the model\n",
        "        tokenizer_name (str): the name of the tokenizer\n",
        "        batch_size (int): the batch size\n",
        "        device (torch.device): the device to run the inference on\n",
        "\n",
        "    Returns:\n",
        "        dict: a dictionary of losses for each language\n",
        "    \"\"\"\n",
        "    print(f\"Running inference for model {model_name}\")\n",
        "    tokenizer = Tokenizer(tokenizer_name)\n",
        "    flores_dataloaders = build_dataloaders(LANGUAGES, batch_size, collate_fn, tokenizer)\n",
        "\n",
        "    model = build_model(model_name, device)      \n",
        "    model.eval()\n",
        "\n",
        "    losses = {lang: [] for lang in LANGUAGES}  # store per-batch losses for each language\n",
        "\n",
        "    for idx_language, language in enumerate(LANGUAGES):\n",
        "        print(f\"Calculating losses for language {language}\")\n",
        "        for idx_batch, batch in enumerate(tqdm(flores_dataloaders[language])):\n",
        "          if idx_language == 0 and idx_batch == 0:\n",
        "            print(f\"PRINTING TOKENIZED DATA:\\n {batch}\")\n",
        "          \n",
        "          # https://github.com/huggingface/transformers/blob/94b3f544a1f5e04b78d87a2ae32a7ac252e22e31/src/transformers/models/xglm/modeling_xglm.py#L915\n",
        "          # If labels are provided, the model will return the loss in the outputs\n",
        "          outputs = model.forward(**batch.to(device))\n",
        "          losses[language].append(outputs.loss.cpu())\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Util method for visualizing the loss for each langauge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Some plot configuration\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "# Credits: https://www.futurile.net/2016/02/27/matplotlib-beautiful-plots-with-style/\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "plt.rcParams['font.serif'] = 'Ubuntu'\n",
        "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.labelsize'] = 10\n",
        "plt.rcParams['axes.labelweight'] = 'bold'\n",
        "plt.rcParams['axes.titlesize'] = 10\n",
        "plt.rcParams['xtick.labelsize'] = 8\n",
        "plt.rcParams['ytick.labelsize'] = 8\n",
        "plt.rcParams['legend.fontsize'] = 10\n",
        "plt.rcParams['figure.titlesize'] = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### XGLM loss for each language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "abzpTjDNfNsX",
        "outputId": "523b0b97-96f6-4055-9f85-1f288b3bc626"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 2\n",
        "xglm_losses = inference(\"facebook/xglm-564M\", \"facebook/xglm-564M\", BATCH_SIZE, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting the losses\n",
        "fig, axes = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "# create a bar plot for each langauge\n",
        "for i, (language, loss) in enumerate(xglm_losses.items()):\n",
        "    mean = np.mean(loss)  \n",
        "    axes.bar(i, mean, label=language)\n",
        "    plt.text(i, mean, f\"{mean:.2f}\", ha=\"center\", va=\"bottom\")\n",
        "\n",
        "# Format plot\n",
        "axes.grid(which='major', color='#EEEEEE', linestyle='-', linewidth=0.5)\n",
        "axes.set_axisbelow(True)\n",
        "axes.set_xlabel(\"Language\") # x-axis label\n",
        "axes.set_xticks(range(len(LANGUAGES))) # x-axis ticks\n",
        "axes.set_xticklabels(xglm_losses.keys()) # x-axis tick labels\n",
        "axes.set_ylabel(\"Mean loss\") # y-axis label\n",
        "axes.set_ylim(0, 9) # range of y-axis\n",
        "axes.set_title(f\"XGLM-564M mean language model loss\"); # title\n",
        "\n",
        "##########################################################################\n",
        "# Output stored in /data/task_1/charts/xglm_mean_language_model_loss.png #\n",
        "##########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing XGLM to GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LANGUAGES.append(\"als_Latn\")\n",
        "\n",
        "flores_data = load_flores_datasets(LANGUAGES, AVAILABLE_SPLITS)\n",
        "\n",
        "xglm_losses = inference(\"facebook/xglm-564M\", \"facebook/xglm-564M\", BATCH_SIZE, device)\n",
        "gpt2_losses = inference(\"gpt2\", \"gpt2\", BATCH_SIZE, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plottin the losses\n",
        "width = 0.40\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "idx = 0\n",
        "for (lang, xglm_loss), (_, gpt2_loss) in zip(xglm_losses.items(), gpt2_losses.items()):\n",
        "    xglm_mean_loss = np.mean(xglm_loss)  \n",
        "    gpt2_mean_loss = np.mean(gpt2_loss)  \n",
        "\n",
        "    axes.bar(idx - 0.2, xglm_mean_loss, label=language, width=width, color=\"orange\")\n",
        "    axes.bar(idx + 0.2, gpt2_mean_loss, label=language, width=width, color=\"blue\")\n",
        "\n",
        "    plt.text(idx - 0.2, xglm_mean_loss, f\"{xglm_mean_loss:.2f}\", ha=\"center\", va=\"bottom\")\n",
        "    plt.text(idx + 0.2, gpt2_mean_loss, f\"{gpt2_mean_loss:.2f}\", ha=\"center\", va=\"bottom\")\n",
        "\n",
        "    idx += 1\n",
        "\n",
        "# Format plot\n",
        "axes.grid(which='major', color='#EEEEEE', linestyle='-', linewidth=0.5)\n",
        "axes.set_axisbelow(True)\n",
        "axes.set_xlabel(\"Language\") # x-axis label\n",
        "axes.set_xticks(range(len(LANGUAGES))) # x-axis ticks\n",
        "axes.set_xticklabels(xglm_losses.keys()) # x-axis tick labels\n",
        "axes.set_ylabel(\"Mean loss\") # y-axis label\n",
        "axes.set_ylim(0, 12) # range of y-axis\n",
        "axes.set_title(f\"XGLM-564M vs GPT-2 mean language model loss\"); # title\n",
        "axes.legend(['XGLM-564M', 'GPT-2'])\n",
        "\n",
        "##################################################################################\n",
        "# Output stored in /data/task_1/charts/xglm_vs_gpt2_mean_language_model_loss.png #\n",
        "##################################################################################"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
